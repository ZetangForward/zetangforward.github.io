<!DOCTYPE html>
<html lang="en">

<head>
    <title>Zecheng Tang Homepage</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Zecheng Tang's personal homepage.">
    <meta name="keywords" content="Zecheng Tang, ZetangForward, zecheng, AI, NLP, Long Context">
    <link rel="icon" type="image/png" href="assets/icon/logo.png">
    
    <style>
        body {
            background-image: linear-gradient(#e66465, #9198e5);
            background-attachment: fixed;
            margin: 0;
            font-family: 'Georgia', 'Times New Roman', serif;
            color: #333;
            -webkit-text-size-adjust: 100%;
        }

        a {
            color: #1a0dab; 
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }

        .container {
            max-width: 550pt; 
            margin: 30px auto;
            border: 1px solid gray;
            padding: 25px 40px;
            background: white;
            box-shadow: 1px 1px 10px #22222277;
            font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
            line-height: 1.5;
            font-size: 15px;
        }

        h1 {
            font-family: 'Georgia', serif;
            margin-bottom: 5px;
            font-size: 2.2em;
        }

        h2 {
            font-family: 'Georgia', serif;
            font-size: 1.4em;
            margin-top: 25px;
            margin-bottom: 10px;
            color: #000;
        }

        hr {
            border: 0;
            border-top: 1px solid #ccc;
            margin: 20px 0;
        }

        .nav-links {
            font-size: 0.95em;
        }
        
        .nav-links a {
            margin: 0 3px;
        }

        .portrait {
            float: right;
            width: 100px;         
            height: auto;         
            margin-left: 25px;
            margin-bottom: 15px;
            margin-top: 10px;
            border: 1px solid #ccc; 
            padding: 4px;          
            background: white;
            box-shadow: 2px 2px 5px rgba(0,0,0,0.15);
        }


        .experience-grid {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between; 
        }
        
        .experience-item {
            width: 48%; 
            margin-bottom: 20px;
            box-sizing: border-box;
        }

        .exp-title {
            font-weight: bold;
            color: #000;
            display: block;
        }

        .exp-role {
            font-style: italic;
            color: #444;
        }

        .exp-rep {
            display: block;
            font-size: 0.88em; 
            color: #666;      
            margin-top: 4px;
            line-height: 1.4;
        }

        .exp-rep a {
            color: #666 !important;
            text-decoration: underline !important;
            text-underline-offset: 2px;
        }
        .exp-rep a:hover {
            color: #444 !important;
            text-decoration: underline !important;
        }
        
        
        @media (max-width: 600px) {
            .portrait {
            float: none;
            display: block;
            margin: 0 auto 20px auto;
            width: 160px;
            }
            .container {
            padding: 15px 20px;
            margin: 0 8px;
            border: none;
            }
            .experience-item {
            width: 100%; 
            margin-bottom: 25px;
            }
        }

        ul {
            padding-left: 20px;
            margin-top: 5px;
        }

        li {
            margin-bottom: 8px;
        }


        .me {
            font-weight: 500; /*  normal, lighter, bolder, 100-900 */
            color: #000;
            text-decoration: underline;
            font-style: italic;
        }
        
        .date-badge {
            font-weight: bold;
            color: #555;
            font-size: 0.9em;
        }

        .links-list li {
            list-style-type: none;
            margin-left: -20px;
        }
    </style>
</head>

<body>

    <div class="container">
        <img class="portrait" src="assets/img/profile.jpg" alt="Zecheng Tang" />
        <h1>Zecheng Tang (汤泽成)</h1>
        
        <hr />
        
        <div class="nav-links">
            Email: <a href="mailto:zecheng.tang@foxmail.com">zecheng.tang@foxmail.com</a><br/>
            GitHub & Project: <a href="https://github.com/ZetangForward" target="_blank">https://github.com/ZetangForward</a> | <a href="https://github.com/LCM-Lab/" target="_blank">https://github.com/LCM-Lab</a>

        </div>
        
        <hr />
        
        <div class="nav-links">
            <a href="#bio">Biography</a> |
            <a href="#experience">Experience</a> |
            <a href="#publications">Publications</a> |
            <a href="#awards">Honors</a> |
            <a href="#talk">Talks</a> |
            <a href="#service">Service</a> |
            <a href="https://scholar.google.com/citations?user=HUDkBMUAAAAJ" target="_blank">Google Scholar</a> |
            <a href="assets/personal_cv.pdf" target="_blank">CV (PDF)</a> 
        </div>

        <hr />

        <h2 id="bio">Brief Bio</h2>

        <p>
            I am currently a fourth-year PhD candidate at Soochow University (<em>exp. grad. Jul 2027</em>), supervised by <a href="https://lijuntaopku.github.io/" target="_blank">Assoc. Prof. Juntao Li</a> and <a href="https://zhangminsuda.github.io/Pesonel-Information/" target="_blank">Prof. Min Zhang</a>.
            My research focuses on <strong>Machine Learning</strong> and <strong>Foundation Model (with Algorithm-Architecture co-design)</strong>.
            My contributions include but not limited to:
            <a href="https://github.com/QwenLM/Qwen-Image" target="_blank">Qwen-Image</a>, 
            <a href="https://github.com/chenfei-wu/TaskMatrix" target="_blank">Visual ChatGPT</a>, 
            <a href="https://github.com/ProjectNUWA" target="_blank">NUWA</a>-series (<a href="https://github.com/ProjectNUWA/LayoutNUWA" target="_blank">LayoutNUWA</a>/<a href="https://github.com/ProjectNUWA/StrokeNUWA" target="_blank">StrokeNUWA</a>), 
            and <a href="https://github.com/OpenNLG" target="_blank">OpenBA</a>-series (<a href="https://github.com/OpenNLG/OpenBA" target="_blank">OpenBA-V1</a>/<a href="https://github.com/OpenNLG/OpenBA-v2" target="_blank">OpenBA-V2</a>).
        </p>
        <p>
            I have been also conducting research on algorithm optimization (model architecture, post-training) and infrastructure building (evaluation, training platforms) for <strong>scalable Long Sequence Modeling and Aligning</strong> at <a href="https://github.com/LCM-Lab/" target="_blank">LCM-Lab</a>, with <a href="https://github.com/LCM-Lab/" target="_blank">9 publications (currently)</a> in this field.
            See my <a href="https://github.com/LCM-Lab" target="_blank">project homepage</a> for a complete list.
        </p>

        <p>
            Before that, I obtained my Bachelor's degree in Software Engineering from Soochow University (ranked 1st / 380).
            My formal name is Zecheng Tang (汤泽成), and you may call me <em>Zecheng (/zə'tʃɛŋ/)</em>, or simply <em>Tang</em>.
        </p>

        <hr />

        <h2 id="experience">Work Experience</h2>
        
        <div class="experience-grid">
            <div class="experience-item">
                <span class="date-badge">2025.02 - 2026.01</span><br/>
                <span class="exp-title"><a href="https://github.com/QwenLM" target="_blank">Alibaba Cloud (Tongyi-Qwen)</a></span>
                <div class="exp-role">Research Intern</div>
                <span class="exp-rep">
                    Rep. Work: <strong><a href="https://github.com/QwenLM/Qwen-Image" target="_blank">Qwen-Image</a></strong> (tech report)<br/>
                    (Foundation Generative Model & AI-Infra & Post-training)
                </span>
            </div>

            <div class="experience-item">
                <span class="date-badge">2024.10 - 2025.01</span><br/>
                <span class="exp-title"><a href="https://opendatalab.github.io/index.html" target="_blank">Shanghai AI Lab (OpenDataLab)</a></span>
                <div class="exp-role">Academic Collaboration</div>
                <span class="exp-rep">
                    Rep. Work: <strong><a href="https://github.com/LCM-Lab/context-denoising-training" target="_blank">CDT</a></strong> (ICLR 2026)<br/>
                    (Long-context Modeling and Optimization)
                </span>
            </div>

            <div class="experience-item">
                <span class="date-badge">2024.05 - 2024.09</span><br/>
                <span class="exp-title"><a href="https://platform.stepfun.ai/" target="_blank">StepFun AI (Sora Team)</a></span>
                <div class="exp-role">Research Intern</div>
                <span class="exp-rep">
                    Rep. Work: <strong><a href="https://github.com/stepfun-ai/Step-Video-T2V" target="_blank">Step-Video-T2V</a></strong> (tech report) <br/>
                    (Video Generation & Evaluation)
                </span>
            </div>

            <div class="experience-item">
                <span class="date-badge">2023.01 - 2024.02</span><br/>
                <span class="exp-title"><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank">Microsoft Research Asia (NLC Group)</a></span>
                <div class="exp-role">Research Intern</div> 
                <span class="exp-rep">
                    Rep. Work: <strong><a href="https://github.com/chenfei-wu/TaskMatrix" target="_blank">Visual ChatGPT</a></strong> (Github Star 34K), <strong><a href="https://github.com/ProjectNUWA/LayoutNUWA" target="_blank">LayoutNUWA</a></strong> (ICLR2024), <strong><a href="https://github.com/ProjectNUWA/StrokeNUWA" target="_blank">StrokeNUWA</a></strong> (ICML2025)<br/>
                    (Symbolic Representation in Multimodal LLMs)
                </span>
            </div>
        </div>
        <hr />

        <h2 id="publications">Selected Conference & Journal</h2>
        <p>
            * indicates equal contribution; Full list: <a href="https://scholar.google.com/citations?user=HUDkBMUAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>; Rank system: <a href="https://www.ccf.org.cn/Academic_Evaluation/By_category/" target="_blank">CCF (China Computer Federation)</a> 
        </p>
        <details>
            <summary style="cursor: pointer; color: #1a0dab; font-weight: bold; margin-bottom: 10px;">
                [Click to view / hide selected list]
            </summary>
        
            <div style="margin-bottom: 25px; border-left: 3px solid #eee; padding-left: 15px;">
                <h3 style="font-size: 1.1em; color: #444; margin-top: 0;"> Long-context Modeling & Evaluation</h3>
                <ul style="list-style-type: circle;">
                    <li>
                        <strong>Revisiting Long-Context Modeling from a Context Denoising Perspective</strong><br/>
                        <span class="me">Zecheng Tang</span>, Baibei Ji, Juntao Li, Lijun Wu, Haijia Gui, Min Zhang.<br>
                        <em>ICLR 2026, CCF A</em>.<br/>
                        <a href="https://arxiv.org/pdf/2510.05862" target="_blank">[Paper]</a> <a href="https://github.com/LCM-Lab/context-denoising-training" target="_blank">[Code]</a>
                        <a href="https://mp.weixin.qq.com/s/P59B3A0HzZdGahhz99ir8g" target="_blank">[Media]</a>
                    </li>
                    <li>
                        <strong>L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?</strong><br/>
                        <span class="me">Zecheng Tang</span>, Keyan Zhou, Juntao Li, Baibei Ji, Jianye Hou, Min Zhang.<br/>
                        <em>ACL 2025 (Long Main), CCF A</em>.<br/>
                        <a href="https://aclanthology.org/2025.acl-long.263/" target="_blank">[Paper]</a> <a href="https://github.com/ZetangForward/L-CITEEVAL" target="_blank">[Code]</a>
                        <a href="https://mp.weixin.qq.com/s?search_click_id=4928788152941659301-1769483507733-7447287231&__biz=MzI3ODgwODA2MA==&mid=2247531784&idx=1&sn=a81aab24b560f7307055acdbac2a6fdc&chksm=ea16694fd54880c64a3e46f0d565072f93e87107bfaed962b9678110ed718c8b4f2d2c9cf481&scene=7#rd" target="_blank">[Media]</a>
                    </li>

                    <li>
                        <strong>LOGO -- Long cOntext aliGnment via efficient preference Optimization</strong><br/>
                        <span class="me">Zecheng Tang</span>, Zechen Sun, Juntao Li, Qiaoming Zhu, Min Zhang.<br/>
                        <em>ICML 2025, CCF A</em>.<br/>
                        <a href="https://openreview.net/pdf?id=FSlfoBIctk" target="_blank">[Paper]</a> <a href="https://github.com/ZetangForward/LCM_Stack" target="_blank">[Code]</a>
                        <a href="https://mp.weixin.qq.com/s?search_click_id=16977432080700696624-1769483453438-9813325931&__biz=MzI3ODgwODA2MA==&mid=2247533305&idx=1&sn=7efc5ece630a0cafa138f94281276913&chksm=ea5d456581d7c698b926f11137499866dadb15e55b5b8f3c000355b99058ca136818b67b7d85&scene=7#rd" target="_blank">[Media]</a>
                    </li>

                    <li>
                        <strong>Open-ended long text generation via masked language modeling</strong><br/>
                        Xiaobo Liang*, <span class="me">Zecheng Tang*</span>, Juntao Li, Min Zhang.<br/>
                        <em>ACL 2023 (Long Main), CCF A</em>.<br/>
                        <a href="https://aclanthology.org/2023.acl-long.13.pdf" target="_blank">[Paper]</a> <a href="https://github.com/ZetangForward/OpenLTG-MLM" target="_blank">[Code]</a>
                    </li>
                </ul>
            </div>

            <div style="margin-bottom: 25px; border-left: 3px solid #eee; padding-left: 15px;">
                <h3 style="font-size: 1.1em; color: #444; margin-top: 0;"> Generative Model</h3>
                <ul style="list-style-type: circle;">
                    <li>
                        <strong>StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis</strong><br/>
                        <span class="me">Zecheng Tang</span>, Chenfei Wu, Zekai Zhang, Mingheng Ni, Juntao Li, Nan Duan, et al.<br/>
                        <em>ICML 2024, CCF A</em>.<br/>
                        <a href="https://openreview.net/pdf?id=eVlx8DaG9h" target="_blank">[Paper]</a> <a href="https://github.com/ProjectNUWA/StrokeNUWA" target="_blank">[Code]</a>
                        <a href="https://x.com/_akhaliq/status/1752553683710054521" target="_blank">[Media]</a>
                    </li>
                    <li>
                        <strong>LayoutNUWA: Revealing the hidden layout expertise of large language models</strong><br/>
                        <span class="me">Zecheng Tang</span>, Chenfei Wu, Juntao Li, Nan Duan.<br/>
                        <em>ICLR 2024, CCF A</em>.<br/>
                        <a href="https://arxiv.org/abs/2309.09506" target="_blank">[Paper]</a> <a href="https://github.com/ProjectNUWA/LayoutNUWA" target="_blank">[Code]</a>
                        <a href="https://x.com/_akhaliq/status/1703969969728295354" target="_blank">[Media]</a>
                    </li>
                </ul>
            </div>

            <div style="margin-bottom: 25px; border-left: 3px solid #eee; padding-left: 15px;">
                <h3 style="font-size: 1.1em; color: #444; margin-top: 0;"> Foundation Model (& Optimization)</h3>
                <ul style="list-style-type: circle;">
                    <li>
                        <strong>OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model</strong><br/>
                        Juntao Li*, <span class="me">Zecheng Tang*</span>, Yuyang Ding*, Pinzheng Wang*, Min Zhang, et al.<br/>
                        <em>SCIS 2025, CCF A</em>.<br/>
                        <a href="http://scis.scichina.com/en/2025/192103.pdf" target="_blank">[Paper]</a> <a href="https://github.com/OpenNLG/OpenBA" target="_blank">[Code]</a>
                        <a href="https://x.com/_akhaliq/status/1704313528880881976" target="_blank">[Media]</a>
                    </li>

                    <li>
                        <strong>CMD: a framework for Context-aware Model self-Detoxification</strong><br/>
                        <span class="me">Zecheng Tang</span>, Keyan Zhou, Juntao Li, Yuyang Ding, Pinzheng Wang, Min Zhang, et al.<br/>
                        <em>EMNLP 2024 (Long Main), CCF B</em>.<br/>
                        <a href="https://aclanthology.org/2024.emnlp-main.115.pdf" target="_blank">[Paper]</a> <a href="https://github.com/ZetangForward/CMD-Context-aware-Model-self-Detoxification" target="_blank">[Code]</a> 
                        <a href="https://mp.weixin.qq.com/s?search_click_id=9806010725570851076-1769483165451-9253875013&__biz=MzI3ODgwODA2MA==&mid=2247521786&idx=2&sn=4195426b1cd79f30adff8d69cf85b60f&chksm=ea2e6a3303f4d7665ec82396e69f9135bd70876f222107511a9e8dd00f0698a5e5a78e8b48af&scene=7#rd" target="_blank">[Media]</a>
                    </li>

                    <li>
                        <strong>Improving temporal generalization of pre-trained language models with lexical semantic change</strong><br/>
                        Zhaochen Su*, <span class="me">Zecheng Tang*</span>, Xinyan Guan, Juntao Li, Lijun Wu, Min Zhang.<br/>
                        <em>EMNLP 2022 (Long Main), CCF B</em>.<br/>
                        <a href="https://aclanthology.org/2022.emnlp-main.428.pdf" target="_blank">[Paper]</a> <a href="https://github.com/zhaochen0110/LMLM" target="_blank">[Code]</a>
                    </li>
                </ul>
            </div>
        
        </details>
        <hr />

        <h2 id="preprints">Selected Preprint</h2>
        <p>
            * indicates equal contribution; Full list: <a href="https://scholar.google.com/citations?user=HUDkBMUAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>.
        </p>  
        <details>
            <summary style="cursor: pointer; color: #1a0dab; font-weight: bold; margin-bottom: 10px;">
                [Click to view / hide selected list]
            </summary>

            <div style="margin-bottom: 25px; border-left: 3px solid #eee; padding-left: 15px;">
                <h3 style="font-size: 1.1em; color: #444; margin-top: 0;"> Technical Report</h3>
                <ul style="list-style-type: circle;">
                    <li>
                        <strong>Qwen-Image Technical Report</strong><br/>
                        <span class="me">Core Contributor*</span><br> 
                        [Aug 2025] <a href="https://arxiv.org/pdf/2508.02324" target="_blank">[Arxiv]</a> <a href="https://github.com/QwenLM/Qwen-Image" target="_blank">[Code]</a> <a href="https://mp.weixin.qq.com/s/0H_01R8UwZbJVfxWG9zNSg" target="_blank">[Media]</a>
                    </li>
                    <li>
                        <strong>Step-Video-T2V Technical Report</strong><br/>
                        <span class="me">Contributor*</span><br>
                        [Feb 2025] <a href="https://arxiv.org/pdf/2502.10248" target="_blank">[Arxiv]</a> <a href="https://github.com/stepfun-ai/Step-Video-T2V" target="_blank">[Code]</a> <a href="https://mp.weixin.qq.com/s/Rz4CYNi8oSp_kRqNfWtSfA" target="_blank">[Media]</a>
                    </li>
                    <li>
                        <strong>Visual chatgpt: Talking, drawing and editing with visual foundation models</strong><br/>
                        Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, <span class="me">Zecheng Tang</span>, Nan Duan.<br/>
                        [Mar 2023] <a href="https://arxiv.org/pdf/2303.04671" target="_blank">[Paper]</a> <a href="https://github.com/chenfei-wu/TaskMatrix" target="_blank">[Code] </a> <a href="https://mp.weixin.qq.com/s/oanSkopLM93Krx2jVozR_A" target="_blank">[Media]</a>
                    </li>
                </ul>   
            </div>

            <div style="margin-bottom: 25px; border-left: 3px solid #eee; padding-left: 15px;">
                <h3 style="font-size: 1.1em; color: #444; margin-top: 0;"> Long-context Modeling & Evaluation</h3>
                <ul style="list-style-type: circle;">
                <li>
                    <strong>Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</strong><br/>
                    <span class="me">Zecheng Tang</span>, Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Juntao Li, Min Zhang, et al.<br>
                    [Jan 2026] <a href="https://arxiv.org/pdf/2510.05862" target="_blank">[Arxiv]</a> 
                    <a href="https://github.com/LCM-Lab/Elastic-Attention" target="_blank">[Code] </a>
                    <a href="https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247546584&idx=1&sn=93bf4357bba7412c7c405dda72d40230&chksm=eafc8338e2572175ef7047dbaa06a40f063e35bff9549ff666dc74572a398e1f0f8525e2d80c&scene=126&sessionid=1769527116&subscene=91&clicktime=1769527118&enterid=1769527118#rd" target="_blank">[Media]</a>
                </li>
                <li>
                    <strong>MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models</strong><br/>
                    <span class="me">Zecheng Tang</span>, Baibei Ji, Ruoxi Sun, Haitian Wang, Juntao Li, Min Zhang, et al.<br>
                    [Jan 2026] <a href="https://arxiv.org/pdf/2601.11969" target="_blank">[Arxiv]</a> <a href="https://github.com/LCM-Lab/MemRewardBench" target="_blank">[Code] </a> <a href="https://mp.weixin.qq.com/s/3IEahCsyl7gEDtZsivxP4Q" target="_blank">[Media]</a>
                </li>
                <li>
                    <strong>MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models</strong><br/>
                    Keyan Zhou, <span class="me">Zecheng Tang</span>, Libo Qin, Juntao Li, Min Zhang, et al.<br>
                    [Oct 2025] <a href="https://arxiv.org/pdf/2510.13276" target="_blank">[Arxiv]</a> <a href="https://github.com/jiqimaoke/MMLongCite" target="_blank">[Code]</a>
                </li>
                <li>
                    <strong>LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling</strong><br/>
                    <span class="me">Zecheng Tang</span>, Baibei Ji, Quantong Qiu, Haitian Wang, Xiaobo Liang, Juntao Li, Min Zhang.<br>
                    [Oct 2025] <a href="https://arxiv.org/pdf/2510.06915" target="_blank">[Arxiv]</a> <a href="https://github.com/LCM-Lab/LongRM" target="_blank">[Code] </a> <a href="https://mp.weixin.qq.com/s/hGelltIMLxAagZ2qTO9onA" target="_blank">[Media] </a>
                </li>
                
                <li>
                    <strong>LOOM-Scope: LOng-cOntext Model evaluation framework</strong><br/>
                    <span class="me">Zecheng Tang</span>, Haitian Wang, Quantong Qiu, Baibei Ji, Ruoxi Sun, Keyan Zhou, Juntao Li, Min Zhang<br> 
                    [Jul 2025] <a href="https://arxiv.org/abs/2507.04723" target="_blank">[Arxiv]</a> <a href="https://mp.weixin.qq.com/s/CY_aDcE5RpXnQaFiG9YhyQ" target="_blank">[Code] </a> <a href="https://github.com/LCM-Lab/LOOM-Eval" target="_blank">[Media]</a>
                </li>
                </ul>
            </div>
        
        
        </details>
        <hr />

        <h2 id="awards">Honor & Award</h2>
        <p>
            Key terminology translation reference: <a href="https://www.xoveexu.com/project/fund-translation" target="_blank">Chinese Fund Translation.</a> 
        </p>
        <ul>
            <li>2025: <strong>Young Elite Scientists Sponsorship Program (Doctoral Student Special Plan)</strong>, China Association for Science and Technology <a href="https://english.cast.org.cn/" target="_blank">(CAST)</a></li>
            <li>2025: <strong>Top Reviewer</strong>, NeurIPS Program Committee.</li>
            <li>2025: <strong>National Scholarship</strong>, Soochow University.</li>
            <li>2024: <strong>Star of Tomorrow</strong>, Microsoft Research Asia.</li>
            <li>2022: <strong>Outstanding Graduate (ranked 1st/380)</strong>, Soochow University.</li>
            <li>2022: <strong>Huawei Scholarship (Top 5% Undergraduate)</strong>, Huawei Inc.</li>
        </ul>
        <hr />

        <h2 id="talk">Talk</h2>
        <ul>
            <li>[Nov 2024] <strong>Long Context Modeling in LLMs: Advances and Challenges</strong>, NLPCC 2024 Tutorial. <a href="https://github.com/LCM-Lab/Awesome-Long-Context-Model-Source-Collection/blob/main/NLPCC2024-Long_context_model.pdf" target="_blank">[Slides]</a></li>
            <li>[Apr 2023] <strong>Leveraging Large Language Models for Tool Invocation</strong>, OPPO (closed-door seminar).</li>
        </ul>

        <hr />

        <h2 id="service">Service</h2>
        I have been serving as a reviewer for the following conferences:
        <ul>
            <li><strong>Natural Language Processing</strong>: {ACL, EMNLP, ARR} (2022 - 2026), NLPCC (2023)</li>
            <li><strong>Machine Learning</strong>: {ICML, NeurIPS, ICLR} (2024 - 2026)</li>
            <li><strong>Computer Vision</strong>: CVPR (2025, 2026), ICCV (2026)</li>
            <li><strong>Artificial Intelligence</strong>: AAAI (2022, 2024)</li>
        
        </ul>

        <hr />
        <div style="text-align: right; font-size: 0.9em; color: #888;">
            Homepage last updated: 2026-01 | CV last updated: 2025-12 | Template adapted from <a href="https://dpkingma.com/" target="_blank">dpkingma.com</a>
        </div>

    </div>

    <script>
        window.addEventListener('scroll', () => {
            const scrollTop = window.scrollY; 
            const docHeight = document.documentElement.scrollHeight - window.innerHeight;
            const scrollPercent = scrollTop / docHeight; 
            const hueShift = scrollPercent * 90; 
            const startHue = 359 - hueShift; 
            const endHue = 236 - hueShift;
            document.body.style.backgroundImage = `linear-gradient(hsl(${startHue}, 70%, 75%), hsl(${endHue}, 68%, 80%))`;
        });
    </script>

</body>
</html>
